#1.Basic Data Preprocessing For GenAI
import numpy as np
from sklearn.preprocessing import MinMaxScaler

data=np.random.randint(0,255,(10,2))
print('Original data is :-\n',data)

scaler=MinMaxScaler()
data_scaled=scaler.fit_transform(data)
print('Transformed Scaled data is :- \n',data_scaled)



#2.Visualizing Data Distribution for GenAI
import matplotlib.pyplot as plt
import numpy as np

data1=np.random.normal(loc=50,scale=15,size=100)
data2=np.random.normal(loc=150,scale=20,size=100)

plt.hist(data1,label='Group1')
plt.hist(data2,label='Group2')
plt.title('Data Distribution')
plt.xlabel('Data Points')
plt.ylabel('Frequency')
plt.legend()
plt.show()


#3.TensorFlow Computation Graph with Eager Execution
import tensorflow as tf

a=tf.constant([3,5])
b=tf.constant([4,7])
c=a+b
print('Eager Execution output is :- ',c.numpy())

@tf.function
def multiply(x,y):
    return x*y

result=multiply(a,b)
print('Output of Graph Mode is:-',result)



#4.Word2Vec Embeddings
from gensim.models import Word2Vec

sentence=[['He','is','a','boy','his','name','is','Om']]
model=Word2Vec(sentence,window=4,min_count=1,vector_size=5,sg=100)

print('Vectors for word he :- ',model.wv['He'])
print('Similar words to He :- \n',model.wv.most_similar('He'))
print('Similar words to He :- \n',model.wv.similarity('He','his'))
                                        


#5.Glove Pretrained Embeddings

import gensim.downloader as api
model=api.load('glove-wiki-gigaword-50')
print('\nEmbeddings similar for word car\n\n\t',model['car'])
print('\nSimilarity between words car & vehicle \n\n\t',model.similarity('car','vehicle'))
print('\nSimilar words to Car :- \n\n\t',model.most_similar('car'))





#6. BERT Embeddings with Transformers

from transformers import BertTokenizer,BertModel

tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')
model=BertModel.from_pretrained('bert-base-uncased')

input=tokenizer('GenAI can create realistic images',return_tensors='pt')
output=model(**input)

print('Shape of the embedding is :- ',output.last_hidden_state.shape)
print('First Embeddings is :=',output.last_hidden_state[0][0][:5])



#7. FAISS Similarity Search
!pip install faiss-cpu

import faiss
import numpy as np

data=np.random.random((5,4)).astype('float32')
index=faiss.IndexFlatL2(4)
index.add(data)

query=np.random.random((1,4)).astype('float32')
distances,indices=index.search(query,k=5)

print('Query is',query)
print('Index is',indices)
print('Distances',distances)



#8. Self-Attention Mechanism
import torch
import torch.nn.functional as F

x = torch.rand(1, 3, 4)
Q, K, V = x, x, x

scores = torch.matmul(Q, K.transpose(-2, -1)) / (4 ** 0.5)
weights = F.softmax(scores, dim=-1)
output = torch.matmul(weights, V)

print("Attention Weights:", weights)
print("Output:", output)




#9. Simulating Diffusion Denoising
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter

image=np.random.rand(24,24)
plt.imshow(image)
plt.title('Step0 Noisy Image')
plt.show()

for step in range(1,4):
    image=gaussian_filter(image,sigma=1)
    plt.imshow(image)
    plt.title(f'Denoising Step {step}')
    plt.show()




#10. FID Calculation 
from scipy.linalg import sqrtm
import numpy as np

def calculate_fid(mu1, sigma1, mu2, sigma2):
    diff = mu1 - mu2
    covmean = sqrtm(sigma1.dot(sigma2))
    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)
    return np.real(fid)

mu1, sigma1 = np.random.rand(3), np.eye(3)
mu2, sigma2 = np.random.rand(3), np.eye(3)

print("FID Score:", calculate_fid(mu1, sigma1, mu2, sigma2))



























